# Convolutional Neural Networks


We will start with...

```{r cnn_init}
library(generics)
library(keras)
library(tensorflow)
```

## 3.1 Image Classifications with MLP
### 3.1.1 Input Layer

Let's start by building a model that will begin by flattening a 28x28 image matrix into a vector.

```{r start_minst_mod}
model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(28,28))
```  

Great, now let's add some hidden layers.

### 3.1.2 Hidden Layers

We want to add two dense layers with 512 nodes.

```{r add_dense_nodes}
model %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 512, activation = "relu")

```

### 3.1.3 Output Layer

Now we need to add an output layer using softmax for the numbers 1-10.

```{r output_layer_minst}
model %>%
  layer_dense(units = 10, activation = "softmax")
```


To see the whole model together - and what it produces

```{r minst_simple_mod}
#sequential model
model <- keras_model_sequential() %>%

  #flatten the input
  layer_flatten(input_shape = c(28,28)) %>%
  
  #two hidden layers
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 512, activation = "relu")%>%
  
  #the output layer
  layer_dense(units = 10, activation = "softmax")

summary(model)
```

### 3.1.3 BONUS - fit

First, let's grab the mnist from keras. We'll regularize it to be between 0 and 1 to speed up convergence, and use `to_categorical()` for the labels so that we can use softmax

```{r minst_fit_simple}
# fetch the data set from the the keras library
mnist <- dataset_mnist()

train_images <- mnist$train$x / 255 #some regularization
train_labels <- to_categorical(mnist$train$y)

test_images <- mnist$test$x / 255 #some regularization
test_labels <- to_categorical(mnist$test$y)
```

What do these images look like?

```{r see_image}
plot_mnist <- function(x){
    train_images[x,,] %>% 
    as.raster(max = 1) %>% 
    plot()
}

par(mfrow = c(2,2))
for(i in c(1,16,72,95)) plot_mnist(i)
par(mfrow = c(1,1))
```

Let's compile the network using stochastic gradient descent as our optimizer, examining the cateogircal crossentropy as our loss function and output the accuracy. 

```{r mnist_compile_simple}
model %>%
  compile(
    optimizer = "rmsprop", #optimization algorithm
    loss = "categorical_crossentropy",
    metrics = c("accuracy")
  )
```

Great! So.... let's fit it!

```{r fit_simple_mnist, message=TRUE, cache = TRUE, cache.path='cache/', results="asis"}
history <- model %>%
  fit(x = train_images,
      y = train_labels,
      epochs = 10,
      batch_size = 128)

plot(history)
```

We can also evaluate this model against test data.

```{r mnist_fit_eval}
model %>% evaluate(test_images, test_labels)
```

Look at that accuracy! Not bad!

Let's look in more detail with `caret` and see how well more model behaves for each number.

```{r caret_confusion, message = FALSE, warning = FALSE}
library(caret)
pred_mnist <- model %>% predict_classes(test_images)

confusionMatrix(data = factor(pred_mnist, levels = 0:9), 
                reference = factor(mnist$test$y, levels = 0:9))
```

Pretty good. Some funkiness with fives and eights, but otherwise, pretty good.

##  3.4 Image Classification using CNNs
### 3.4.1 Building the model architecture

OK, let's build a CNN to analyze MNIST.

```{r model_cnn}
model_cnn <- keras_model_sequential() %>%
  
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                strides = 1, padding = "same",
                activation = "relu", input_shape = c(28,28,1)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  
  layer_conv_2d(filters = 64, kernel_size = c(3,3),
                strides = 1, padding = "same",
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  
  layer_flatten() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax")
```  

So, what does this look like?

```{r modcov_summary}
model_cnn
```


### Bonus: Let's fit this and compare performance

First, we need to reshape the dimensions of the data a bit so that there is one layer (Black and White). Don't worry, we've already rescaled.

```{r reshape_mnist}
train_images_cnn <- array_reshape(train_images, c(nrow(train_images), 28, 28, 1))
test_images_cnn <- array_reshape(test_images, c(nrow(test_images), 28, 28, 1))
```

```{r fit_cnn_mnist, cache = TRUE, cache.path="cache/"}
model_cnn %>%
  compile(
    optimizer = "rmsprop", #optimization algorithm
    loss = "categorical_crossentropy",
    metrics = c("accuracy")
  )

history_cnn <- model_cnn %>%
  fit(x = train_images_cnn,
      y = train_labels,
      epochs = 10,
      batch_size = 128,
      shuffle = TRUE)

plot(history_cnn)
```
We can also evaluate it as before

```{r eval_cnn}
model_cnn %>% evaluate(test_images_cnn, test_labels)
```

Wow. That is a damn site better.

And the confusion matrix and a breakdown by class.

```{r confusion_cnn, message = FALSE, warning = FALSE}
pred_mnist <- model %>% predict_classes(test_images)

confusionMatrix(data = factor(pred_mnist, levels = 0:9), 
                reference = factor(mnist$test$y, levels = 0:9))
```

Also here. Excellent.